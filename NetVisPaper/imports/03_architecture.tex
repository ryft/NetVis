{\color{red} Describe the architecture, how traffic is read and processed, the format of the CSV, why was it designed the way it is? Add diagram}
{\color{red} How real-time is it? What filtering do you support + protocols?}
{\color{red} How modular is the code base, can one simply script in another vis?}

Normalisers:
A normaliser is a class responsible with taking a packet and returning a “normalised” value of a certain attribute of that packet and the other way around. 
Each normalising class is able to create a temporary filter in the application that will filter its corresponding attribute on a certain range. 
Since the normalising class is in control of its filter it creates a zooming effect based on the range of the filter: the lower bound is normalised to 0, the upper bound to 1. 

Flow of Data through the application:
The data feeder creates Packet objects based on the traffic in analyses and feeds it to the Data Controller. 
We implemented a CSV data feeder that reads CSV files (/* maybe here a little more about the structure of the CSV's? */) created with Wireshark but the simple interface of the data feeder makes it easy to extend the application for reading live data.

The Data Controller uses the listener pattern and feeds new packets as they arrive to its listeners (visualizations, analysis panel).
If there are any active filters then the data controller will apply those filters to the data and supply only what is relevant.

When a filter is applied or changed a reset signal is sent from the data controller informing the visualizations that all the data has changed. Along with this signal a list of all the past packets (with filters applied) is provided.

